---
title: "R Notebook for Thermal Conditions Analysis"
output: html_notebook
---

```{r}
##### PRE-AMBLE: 

    #Setup:Pre- (condition 0) and post-retrofit (condition 1) indoor thermal comfort are each dependent on the independent variable outdoor temperature, for each building. 
    # GoalS: 
        # 1) we want to know if pre- and post-retrofit (condition 0 and 1) are significantly different from one another
        # 2) we want to visualize each of the pre- and post-retrofit thermal comfort (on y axis) versus outdoor temp (x axis) on same graph
        # 3) repeat steps 1 and 2 for each building: because they had different retrofit actions and other unmeasured differences
    #Stats approach: have a global model that codes for condition 0 and condition 1 as variables, indoor conditions always in response to outdoor temp variable; then look for 'effect size' of condition 0 versus condition 1... 
    #Other questions to ask of the data... did it impact variance ? within a suite / across suites? (e.g., besides pure mean change...need to check normality/mean/variance of each day across all suites, and check...how to check variance of )
```


```{r} 
##### LIBRARIES:
library(readstata13) #to read in old stata file(s) with thermal and temp data
library(readr) #to read csv weather files
library(tidyverse) #for everything 
library(dplyr) #for everything 
library (lubridate) # for aligning dates
```



```{r}
##### PREP RAW DATA:
##### Outline of steps in this chunk of code:
    # 1) Get outdoor temp by date & clean it up into a dataframe with date, temp 
    # 2) Get indoor thermal comfort by date...Reconcile two different ways of dating outdoor and indoor  
    # 4) Add  "pre"/"post" within a single dataframe with columns: pre/post, outdoor temp, indoor thermal condition, building, suite*, date* 
        # *n.b. suite and date will be useful for quick checks and other possible investigations of interest e.g., matching survey data with thermal conditions 
        


##### Step 1) Get outdoor temp files into one dataframe by date:
WeatherRaw <-
    list.files("./RawData_doNotUpload/weather1", pattern = ".csv", full.names=TRUE, recursive = FALSE) %>%
    lapply(
        read_csv, 
        skip=24, # Skip meta-data rows
        col_types = '_iii_____d_________________') %>% #choose only columns of interest
    bind_rows() 

WeatherRaw <-  #this is done in two steps because some years of weather have different underlying characteristic files
    list.files("./RawData_doNotUpload/weather2", pattern = ".csv", full.names=TRUE, recursive = FALSE) %>%
    lapply(
        read_csv, 
        col_types = '_____iii_____d_________________') %>% #choose only columns of interest
    bind_rows(.,WeatherRaw)  

Check2<- # look at dates of sensorData...because we want to align the weather data with these dates
  SensorRaw %>%
  select(date) %>%
  distinct() # the sensor data skips a couple of days (around new year's...or else the 'week' is 8 days long!)
  
WeatherCleaned <-
    WeatherRaw %>%
    rowwise() %>%
    mutate(
        date = as.Date(
            paste(Year,Month,Day, sep = "-"))) %>%
    select(date,`Mean Temp (°C)`) %>%
    rename(tmp_out_daily =`Mean Temp (°C)`)    # rename to differentiate between indoor/outdoor and daily/weekly average
# n.b. These next steps can be disgarded/edited if sensor data is 'fixed'; the next steps are a 'workaround' because
# the current sensor data is not continuously cut into weeks; rather it re-begins at the beginning of each calendar year,
# which means the current sensor data skips a day at the end of each year
date2015<-
    WeatherCleaned %>%
    subset(year(date) == 2015) %>%
    group_by(
        wks = cut(date, breaks = '7 days')) 
date2016<-
    WeatherCleaned %>%
    subset(year(date) == 2016) %>%
    group_by(
        wks = cut(date, breaks = '7 days')) 
date2017<-
    WeatherCleaned %>%
    subset(year(date) == 2017) %>%
    group_by(
        wks = cut(date, breaks = '7 days'))
date2018<-
    WeatherCleaned %>%
    subset(year(date) == 2018) %>%
    group_by(
        wks = cut(date, breaks = '7 days'))

dates_merged <-
  Reduce(function(x, y) merge(x, y, all=TRUE), list(date2015, date2016, date2017, date2018)) 
     
WeatherCleaned <-
  na.omit(dates_merged) %>%
  group_by(wks) %>%
  summarise(tmp_out_wkly = mean(tmp_out_daily)) %>%
  mutate(
        week_midpt = as.Date(wks) + 3) # to match with sensordata      



##### Step 2) Get indoor conditions by date and building into a dataframe:
SensorRaw <- 
    read.dta13(choose.files()) %>% # Load LTcomplete_weekly_collapsed.dta, which should be in a private folder i.e., NOT PUBLIC on Github
    mutate( 
        murb = substr(locID,1,3)) # Create a column for building
 
Check <- # Data Quick Check: for each building of interest... 
    SensorRaw %>%
    filter(murb %in% c("mr1","mr2","mr6","mr7")) %>%
    group_by(murb) %>%
    summarize(n_obs_allData = sum(!is.na(murb)),# how many measurements?
        n_obs_pre = sum(date %within% interval(ymd("2015-04-01"),ymd("2016-04-01"))), #Pre-retrofit
        n_obs_post = sum(date %within% interval(ymd("2017-02-15"),ymd("2018-02-15"))), #Post-retrofit
        n_suites = n_distinct(locID), # across how many suites?
        n_obs_pre_suite_pre = sum(n_obs_pre/n_suites), # data points per suite pre
        n_obs_pre_suite_post = sum(n_obs_post/n_suites)) # data points per suite pre

SensorCleaned <- # Put the sensor data relevant columns into a new dataframe
    SensorRaw %>%
    select(murb,locID,date,tmp,com_lower,com_upper) %>%
    rename(week_midpt = date, tmp_in_wk = tmp) # distinguish that date is midpoint of a weekly average 
 

  

##### 3) Merge outdoor and indoor data and categorize into pre and post: 
    #  n.b. Pre-retrofit monitoring: weeks 2872 to 2924 (April 1 2015 - April 1 2016)
    #  n.b. Post-retrofit monitoring: weeks 2970 to 3021, (February 15 2017-February 15 2018)
IEQ <- 
    merge(WeatherCleaned,SensorCleaned,by="week_midpt", all.x = TRUE) %>% # outdoor and indoor data into one dataframe
    select(locID,murb,week_midpt,tmp_out_wkly,tmp_in_wk,com_lower,com_upper) %>% #re-order and get rid of extra date info
    mutate(
        retrofit_yes = case_when (week_midpt %within% interval(ymd("2015-04-01"),ymd("2016-04-01"))~ FALSE,
                                  week_midpt %within% interval(ymd("2017-02-15"),ymd("2018-02-15"))~ TRUE,
                                  TRUE ~ NA) #default if not within the specified intervals
    )
 



```

```{r}
# EXTRANNEOUS TIDBITS

sapply(Sensordata,class)

yr_list <- c(2015,2016)
test2 <- subset(WeatherCleaned,FALSE)
test2 <- for (yr in yr_list) {  
    WeatherCleaned %>% subset(year(date)==yr) %>% bind_rows(.,test2)
}

 #example  d %>% mutate( ints = cut(v1 ,breaks = 11)) %>% group_by(ints) 
    
# Can't use cut after the "case_when" call because the very first cut does the entire date range...
# Solution must use cut on only a portion of dates...need to replicate columns for each year, cut them each differently, then reassemble into one column and group_by on that 
WeatherCleaned$week_start <-
    #cut(WeatherCleaned$date,breaks = '7 days')  
    case_when(
            WeatherCleaned$date %within% interval(ymd("2015-01-01"),ymd("2015-12-31")) ~ 2015, 
            WeatherCleaned$date %within% interval(ymd("2016-01-01"),ymd("2016-12-31")) ~ 2016,
            TRUE ~ 2017)

WeatherCleaned$week_start <-
    #cut(WeatherCleaned$date,breaks = '7 days')  
    case_when(
            WeatherCleaned$date %within% interval(ymd("2015-01-01"),ymd("2017-12-31")) ~ cut(
                      WeatherCleaned$date + 0,
                      breaks = '7 days'), 
            WeatherCleaned$date %within% interval(ymd("2016-01-01"),ymd("2016-12-31")) ~ cut(
                      WeatherCleaned$date + 6,
                      breaks = '7 days'), 
            TRUE ~ cut(
                      WeatherCleaned$date + 5,
                      breaks = '7 days'))  

 mutate(
        week_start = case_when( # create a weekly average, to align with midpoint weekly weather station data
                          date %within% interval(ymd("2015-01-01"),ymd("2016-12-31")) ~ group_by(
                                      week_start = cut(
                                            date,
                                            date+0,
                                            breaks = '7 days')),
                          date %within% interval(ymd("2016-01-01"),ymd("2017-12-31")) ~ groub_by(
                                      week_start = cut(
                                            date,
                                            date-1,
                                            breaks = '7 days')),
                          TRUE ~ "placeholder")) %>%
    summarise(tmp_out_wk = mean(tmp_out_daily)) %>%
    mutate(
        week_midpt = as.Date(week_start) + 3) # to match with sensordata                         
      
  
 
  
    group_by(week_start = case_when(   # create a weekly average, to align with midpoint weekly weather station data
                      date %within% interval(ymd("2015-04-01"),ymd("2016-04-01"))~ cut(
                                                                                            week_start,
                                                                                            date +0,
                                                                                            breaks = '7 days'),
                      date %within% interval(ymd("2017-02-15"),ymd("2018-02-15"))~ cut(
                                                                                            week_start,
                                                                                            date +5,
                                                                                            breaks = '7 days'))) %>%      

      

```


Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
