---
title: "R Notebook for Thermal Conditions Analysis"
output: html_notebook
---

```{r}
##### PRE-AMBLE: 

    #Setup:Pre- (condition 0) and post-retrofit (condition 1) indoor thermal comfort are each dependent on the independent variable outdoor temperature, for each building. 
    # GoalS: 
        # 1) we want to know if pre- and post-retrofit (condition 0 and 1) are significantly different from one another in terms of each: 
            # 1A MODELLED THERMAL COMFORT and 
            # 1B SURVEYED TOO HOT/TOO COLD responses
        # 2) we want to visualize each of the pre- and post-retrofit thermal comfort (on y axis) versus outdoor temp (x axis) on same graph
        # 3) repeat goals 1 and 2 for each building: because they had different retrofit actions and other unmeasured differences
    #Stats approach: have a global model that codes for condition 0 and condition 1 as variables, indoor conditions always in response to outdoor temp variable; then look for 'effect size' of condition 0 versus condition 1... 
    #Other questions to ask of the data... did it impact variance ? within a suite / across suites? (e.g., besides pure mean change...need to check normality/mean/variance of each day across all suites, and check...how to check variance of )
```


```{r} 
##### LIBRARIES:  ============
library(readstata13) #to read in old stata file(s) with thermal and temp data
library(readr) #to read csv weather files

library (lubridate) # for aligning dates
library(ggplot2) # for plotting
library(readxl) # for excel survey data files
library(glmmTMB) # for mixed effects model
#library (betareg) # for betaregression of upper_com ~ temperature and retrofit
#library(GGally) # for two-limit obit model
#library(VGAM) # for two-limit obit model
#library(zoib) # for zoib model, note: first needed to install JAGS-4.x from https://sourceforge.net/projects/mcmc-jags/files/JAGS/4.x/Windows/


#library(mgcv) # for mixed effects modelling with beta regression
#library(betareg) # same as above

library(lme4)
library(MASS) # for glmmPQL for mixed effect model with quasibinomial family
library(AICcmodavg)

library(gridExtra) # for formatting summarized AIC scores
#library(jtools) # for formatting output summary, i.e., using summ() of models
 

#load these last so their function names don't get overridden:
library(tidyverse) #for everything 
library(dplyr) #for everything 

```

```{r}
##### GLOBAL VARIABLES e.g., colours ============
colour1 = "#93C47D" # green 
colour2 = "#F6B26B" # orange
```

```{r}
##### IEQ DATA PREP ============
##### Outline of steps in this chunk of code:
    # 1)  Get indoor thermal comfort by date...
    # 2)  Get outdoor temp by date & 
    # 3)  Reconcile way of dating indoor with corresponding outdoor temps
    # 4)  Add  "pre"/"post" within a single dataframe with columns: pre/post, outdoor temp, indoor thermal condition, building, suite*, date* 
    # ...Suite and date will be useful for quick checks and other possible investigations of interest e.g., matching survey data with thermal conditions 
 

##### Step 1) Get indoor conditions by date and building into a dataframe:
dat.sensor <-
    read.dta13(choose.files())  %>% # Choose LTcomplete_weekly_collapsed-daily.dta from a NON-PUBLIC folder
    mutate(
        murb = substr(locID,1,3)) %>% # create a column for buildingID
    rename(
        tmp_in = tmp) %>% # distinguish between indoor and outdoor temperature
    select(
        murb,locID,date,tmp_in,com_lower,com_upper,mrt,rhm) # select only information needed for next steps 

rm(Day_Weather)

##### Step 2) Get outdoor temp files into one dataframe by date:
dat.weather <-
    list.files("./RawData_doNotUpload/weather1", pattern = ".csv", full.names=TRUE, recursive = FALSE) %>%
    lapply(
        read_csv, 
        skip=24, # Skip meta-data rows
        col_types = '_iii_____d_________________') %>% #choose only columns of interest
    bind_rows() 
dat.weather <-  #this is done in two steps because some years of weather have different underlying characteristic files
    list.files("./RawData_doNotUpload/weather2", pattern = ".csv", full.names=TRUE, recursive = FALSE) %>%
    lapply(
        read_csv, 
        col_types = '_____iii_____d_________________') %>% #choose only columns of interest
    bind_rows(.,dat.weather)  

dat.weather <-
    dat.weather %>%
    rowwise() %>%
    mutate(
        date = as.Date(
            paste(Year,Month,Day, sep = "-"))) %>%
    select(date,`Mean Temp (°C)`) %>%
    rename(tmp_out =`Mean Temp (°C)`)    # rename to differentiate between indoor/outdoor 

 

##### Steps 3 & 4) Merge outdoor and indoor data and categorize into pre and post: 
    #  n.b. Pre-retrofit monitoring: weeks 2872 to 2924 (April 1 2015 - April 1 2016)
    #  n.b. Post-retrofit monitoring: weeks 2970 to 3021, (February 15 2017-February 15 2018)
dat.Sensor.Weather <- 
    merge(dat.weather, dat.sensor,by="date", all.x = TRUE, all.y = TRUE) %>% # outdoor and indoor data into one dataframe
    mutate(
        retrofit_yes = case_when (date %within% interval(ymd("2015-04-01"),ymd("2016-03-30"))~ FALSE,
                                  date %within% interval(ymd("2017-02-01"),ymd("2018-01-31"))~ TRUE,
                                  TRUE ~ NA)) %>% #default to NA if date outside the relevant intervals  
    drop_na(retrofit_yes) 
 
  
##### END OF IEQ DATA PREP
```

```{r}
##### QUICK CHECKS ============


Num_suites <-
  dat.Sensor.Weather %>%
  group_by(murb) %>%
  summarize(n_suites = n_distinct(locID))

Check_N_Observations <- # Data Quick Check: for each building of interest... 
    Day_Sensor %>%
    filter(murb %in% c("mr1","mr2","mr6","mr7")) %>%
    group_by(murb) %>%
    summarize(n_obs_allData = sum(!is.na(murb)),# how many measurements?
        n_obs_pre = sum(date %within% interval(ymd("2015-04-01"),ymd("2016-04-01"))), #Pre-retrofit
        n_obs_post = sum(date %within% interval(ymd("2017-02-15"),ymd("2018-02-15"))), #Post-retrofit
        n_suites = n_distinct(locID), # across how many suites?
        n_obs_persuite_pre = sum(n_obs_pre/n_suites), # data points per suite pre
        n_obs_persuite_post = sum(n_obs_post/n_suites)) # data points per suite pre

Check_Pre_Post_count<- # check that we have 365 days per suite of data for appropriate time period
  DF_SensorNWeather %>%
    filter(murb %in% c("mr1","mr2","mr6","mr7")) %>%
    group_by(murb,retrofit_yes) %>%
    summarise(n_days_recorded= n(), start=min(date), end=max(date), n_suites = n_distinct(locID), n_obs_per_suite = n_days_recorded/n_suites )
  

```

```{r}
##### IEQ  MODEL PREP  ============
##### Outline of steps in this chunk of code:
    # 5) Group data by building 
    # 6) Visualize dataset (look for shape of distribution) 
  

##### Step 5)  

# Separate each murb into its own dataframe, for using as model input later:
dat.mr <- list() #Create a list in which I will save all six of the dfs
for (i in 1:7) {
    df_murb<-dat.Sensor.Weather %>%
        filter(murb==(paste0("mr",i)))  #copy all the data for a given building
        dat.mr[[i]] <- df_murb}
rm(df_murb)
rm(i)

##### Step 6)  

# Quick visualization of the distirbution of upper_com values:
dat.Sensor.Weather%>%  
      #subset(murb %in% c("mr1","mr2","mr6","mr7")) %>%
      ggplot(aes(x=com_upper,fill=retrofit_yes)) +
      scale_color_manual(values = c(colour2, colour1)) +
      scale_fill_manual(values = c(colour2, colour1)) +
      geom_histogram(position="dodge", binwidth = 0.05) + # Note: lots of values at truncated edges {0,1} 
      facet_wrap(~murb, nrow = 1) +
      theme(aspect.ratio = 0.8) 

 
# N.B. Based on visualization in step 6, and experimental design, likely a mixed model with a 'special' link function (e.g., not gaussian) will fit best

##### END OF IEQ MODEL PREP
```


```{r}
##### SIX MIXED EFFECTS REGRESSION MODELS 1, 1B, 1C, 2, 3, 4 ============

 
# MODEL 1: BASE MODEL: LME w RANDOM INTERCEPT 
# example from https://ourcodingclub.github.io/2017/03/15/mixed-models.html#second 
# mixed.lmer2 <- lmer(testScore ~ bodyLength2 + (1|mountainRange) + (1|sample), data = dragons) 

m1.lmer<- list() #Create a list in which I will save all models (one per building) of one type (e.g., mixed effect, logit)
m1.summary <- list()#Create a list for each summary of model
for (i in 1:7) {
    model1<- lmer(com_upper ~ tmp_out + retrofit_yes + (1|locID),
                 REML=FALSE,
                 data=dat.mr[[i]])
    m1.lmer[[i]] <- model1 
    m1.summary[[i]]<-summary(model1)
}


# MODEL 1B: SAME AS 1, but with tmp^2 
m1b.lmer<- list() #Create a list in which I will save all models (one per building) of one type (e.g., mixed effect, logit)
m1b.summary <- list()#Create a list for each summary of model
for (i in 1:7) {
    model1<- lmer(com_upper ~ poly(tmp_out,2) + retrofit_yes + (1|locID),
                 REML=FALSE,
                 data=dat.mr[[i]])
    m1b.lmer[[i]] <- model1 
    m1b.summary[[i]]<-summary(model1)
}
rm(model1) 

 
# MODEL 1C: SAME AS 1B, but interaction between retrofit and temp 
m1c.lmer<- list() #Create a list in which I will save all models (one per building) of one type (e.g., mixed effect, logit)
m1c.summary <- list()#Create a list for each summary of model
for (i in 1:7) {
    model1<- lmer(com_upper ~ I(tmp_out^2) + retrofit_yes*tmp_out + (1|locID),
                 REML=FALSE,
                 data=dat.mr[[i]])
    m1c.lmer[[i]] <- model1 
    m1c.summary[[i]]<-summary(model1)
}

rm(model1) 

# MODEL 2: RANDOM INTERCEPT + FAMILY: BINOMIAL
# example from https://www.rdocumentation.org/packages/MASS/versions/7.3-51.4/topics/glmmPQL
# summary(glmmPQL(y ~ trt + I(week > 2), random = ~ 1 | ID, family = binomial, data = bacteria))}


m2.glmer.bin<- list() #Create a list in which I will save all models (one per building) of one type (e.g., mixed effect, logit)

m2.glmer.bin[[1]] <-glmmTMB(com_upper ~ tmp_out + I(tmp_out^2) + retrofit_yes + (1|locID), dat.mr[[1]],
                            family=list(family="binomial",
                                        link="logit"))
m2.glmer.bin[[2]] <-glmmTMB(com_upper ~ tmp_out + I(tmp_out^2) + retrofit_yes + (1|locID), dat.mr[[2]], 
                            family=list(family="binomial",
                                        link="logit"))
m2.glmer.bin[[6]] <-glmmTMB(com_upper ~ tmp_out + I(tmp_out^2) + retrofit_yes + (1|locID), dat.mr[[6]], 
                            family=list(family="binomial",
                                        link="logit"))
m2.glmer.bin[[7]] <-glmmTMB(com_upper ~ tmp_out + I(tmp_out^2) + retrofit_yes + (1|locID), dat.mr[[7]], 
                            family=list(family="binomial",
                                        link="logit"))

# MODEL 3: RANDOM INTERCEPT + FAMILY: QUASIBINOMIAL
# Note: glmmPQL and glmer don't support quasibinomial, only glmmPQL() of package MASS. And random effects syntax is different...  random=~1|boite 
# example from https://statistique-et-logiciel-r.com/introduction-aux-glmm-avec-donnees-de-proportion/
# example glmmPQL1 <- glmmPQL(y~trt, random=~1|boite, family=quasibinomial, data=mydata)

m3.glmer.qb<- list() #Create a list in which I will save all models (one per building) of one type (e.g., mixed effect, logit)
for (i in 1:7) {
    model1<- glmmPQL(com_upper ~ tmp_out + retrofit_yes, random =~ 1|locID, 
                     family=quasibinomial, 
                     data= dat.mr[[i]])
    m3.glmer.qb[[i]] <- model1  
}
rm(model1)

 
# Anova and AIC and AICc not available for this model fit (quasi-binomial) 
overdisp_fun <- function(model) { # from https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html
    rdf <- df.residual(model)
    rp <- residuals(model,type="pearson")
    Pearson.chisq <- sum(rp^2)
    prat <- Pearson.chisq/rdf
    pval <- pchisq(Pearson.chisq, df=rdf, lower.tail=FALSE)
    c(chisq=Pearson.chisq,ratio=prat,rdf=rdf,p=pval)
}

overdisp_fun(m3.glmer.qb) # doesn't work because can't extract the residuals in the usual way... try a custom function:
# Can use AICcCustom(logL, K, return.K = FALSE, second.ord = TRUE, nobs = NULL, c.hat = 1)
# glmmPQL doesn't report the log-likelihood however it is the same as with the binomial family fit, so we will extract the loglikelihood from that and keep the dispersion parameter from the quasi-binomial model 
# logLik = -2668.5, 
AICcCustom(-2668.5, 3, return.K = FALSE, second.ord = TRUE, nobs = 5834, c.hat = 1) # AIC 5343.004
# Can plot the residuals: 
qqnorm(m3.glmer.qb$residuals)
qqline(m3.glmer.qb$residuals)

# Arguments in AICcCustom: 
# logL the value of the model log-likelihood.
# K the number of estimated parameters in the model.
# return.K logical. If FALSE, the function returns the information criterion specified. If TRUE, the function returns K (number of estimated parameters) for a given model.
# second.ord logical. If TRUE, the function returns the second-order Akaike information criterion (i.e., AICc).
# nobs the sample size required to compute the AICc or QAICc.
# c.hat value of overdispersion parameter (i.e., variance inflation factor) such as that obtained from c_hat. Note that values of c.hat different from 1 are only appropriate for binomial GLM’s with trials > 1 (i.e., success/trial or cbind(success, failure) syntax), with Poisson GLM’s, single-season or dynamic occupancy models.  c.hat > 1, AICcCustom will return the quasi-likelihood analogue of the information criterion requested.


 
# MODEL 4: BETA MIXED MODEL (If possible, with Random Intercept! e.g., ...may have to transform data to (0,1)
# example from: https://stats.stackexchange.com/questions/233366/how-to-fit-a-mixed-model-with-response-variable-between-0-and-1 
# example glmmTMB(p ~ a+b+c + (1|subject), myData, family=list(family="beta",link="logit"))}
# To use beta, we must first transform [0,1] to (0,1):  


# first transform the com_upper response variable:
# length(dat.Sensor.Weather$com_upper) is > 28000 and under the transformation it produces numbers too small... let n = 10000
inputData <- 
    dat.Sensor.Weather %>%
        subset(murb=="mr1") %>%
        mutate( # transform y=upper_com s.t. (y * (n−1) + 0.5) / n , where n is the sample size
           trans_com = (com_upper*10000-1)+0.5)/10000  
        

m4.glmer.beta<-glmmTMB(trans_com ~ tmp_out + retrofit_yes + (1|locID), inputData, family=list(family="beta",link="logit"))
summary(model)

 


```


```{r}
#####  MODEL COMPARISON AND EVALUATION: CHOOSE MODEL 1C ============

# Build a table with AIC and QQ plots for each model (3 models and 4 buildings each, also the average AIC for a given model)

# MODEL 1: BASE MODEL: LME w RANDOM INTERCEPT 
df = NULL
for (k in c(1,2,6,7)) {
      model_name = "m1.lmer"
      murb_ID = paste("mr",k)
      AIC_value = AIC(m1.lmer[[k]])
      df = rbind(df, data.frame(model_name,murb_ID,AIC_value))
      qqnorm(resid(m1.lmer[[k]]), 
          main = paste("MODEL 1 (Gaussian) Q-Q Plot Murb",k),
          xlab = "Theoretical Quantiles", 
          ylab = "Sample Quantiles")
      qqline(resid(m1.lmer[[k]]))}


# MODEL 1B: SAME AS 1, but with tmp^2 
for (k in c(1,2,6,7)) {
      model_name = "m1b.lmer"
      murb_ID = paste("mr",k)
      AIC_value = AIC(m1b.lmer[[k]])
      df = rbind(df, data.frame(model_name,murb_ID,AIC_value))
      qqnorm(resid(m1b.lmer[[k]]), 
          main = paste("MODEL 1B (Gaussian Poly) Q-Q Plot Murb",k),
          xlab = "Theoretical Quantiles", 
          ylab = "Sample Quantiles")
      qqline(resid(m1b.lmer[[k]]))}

# MODEL 1C: SAME AS 1B, but with interaction with tmp  
for (k in c(1,2,6,7)) {
      model_name = "m1c.lmer"
      murb_ID = paste("mr",k)
      AIC_value = AIC(m1c.lmer[[k]])
      df = rbind(df, data.frame(model_name,murb_ID,AIC_value))
      qqnorm(resid(m1c.lmer[[k]]), 
          main = paste("MODEL 1C (Gaussian Poly w Interaction Term) Q-Q Plot Murb",k),
          xlab = "Theoretical Quantiles", 
          ylab = "Sample Quantiles")
      qqline(resid(m1c.lmer[[k]]))}

# MODEL 2: SAME AS 1B, but with Binomial family
for (k in c(1,2,6,7)) {
      model_name = "m2.glmer.bin"
      murb_ID = paste("mr",k)
      AIC_value = AIC(m2.glmer.bin[[k]])
      df = rbind(df, data.frame(model_name,murb_ID,AIC_value))
      qqnorm(resid(m2.glmer.bin[[k]]), 
          main = paste("MODEL 2 (Binomial) Q-Q Plot Murb",k),
          xlab = "Theoretical Quantiles", 
          ylab = "Sample Quantiles")
      qqline(resid(m2.glmer.bin[[k]]))}


# MODEL 3: SAME AS 1B, but with Quasi-Binomial family
for (k in c(1,2,6,7)) {
      model_name = "m3.glmer.qb"
      murb_ID = paste("mr",k)
      ll=logLik(m2.glmer.bin[[k]])
      AIC_value = AICcCustom(ll, 3, return.K = FALSE, second.ord = TRUE, nobs = 5834, c.hat = 1)
      df = rbind(df, data.frame(model_name,murb_ID,AIC_value))
      qqnorm(resid(m3.glmer.qb[[k]]), 
          main = paste("MODEL 3 (Quasi-Binomial) Q-Q Plot Murb",k),
          xlab = "Theoretical Quantiles", 
          ylab = "Sample Quantiles")
      qqline(resid(m3.glmer.qb[[k]]))} 


t.AIC.scores <-  # make a table with AIC scores for each model 
  df %>% 
  group_by(model_name) %>%
  summarize(AIC_average = mean(AIC_value)) # (take average across all murbs, for brevity - minimum is same for all murbs)
pdf("data_output.pdf", height=11, width=8.5)
grid.table(t.AIC.scores )
dev.off()

      
# Check residuals if we used a lm   
model_check <- lm(dat.Sensor.Weather$com_upper ~ dat.Sensor.Weather$tmp_out + dat.Sensor.Weather$retrofit_yes) # x is tmp_out and y is com_upper
qqnorm(model_check$residuals)
qqline(model_check$residuals)
hist(model_check$residuals) ## problem is that residuals are not normally distributed


# Check whether 
qqnorm(resid(mixed.lmer1))
qqline(resid(mixed.lmer1)) # doesn't look great at edges... change to a quasi-binomial? 

```


```{r}
# SUMMARIZE AND VISUALIZE RESULTS OF MODEL 1c ============

# SUMMARIZE
summ(m1c.lmer[[5]])
summary(m1c.lmer[[5]])
anova(m1c.lmer[[5]])

# VISUALIZE
ii=5
(mm_plot <- ggplot(dat.mr[[ii]], aes(x = tmp_out, y = com_upper, colour = retrofit_yes)) +
      facet_wrap(~locID, nrow=2) +   # a panel for each suite
      geom_point(alpha = 0.2) +
      theme_classic() +
      geom_line(data = cbind(dat.mr[[ii]], pred = predict(m1c.lmer[[ii]])), aes(y = pred), size = 1) +  # adding predicted line from mixed model 
      theme(legend.position = "bottom") +   
      scale_color_manual(values = c(colour2, colour1))+
      ylim(0,1)
)


```


```{r}
# TRYING OUT VISUALIZATIONS OF MIXED MODELS ============

##########

library(nlme)
fm2 <- lme(distance ~ age + Sex, data = Orthodont, random = ~ 1|Subject)

newdat <- expand.grid(Sex=unique(Orthodont$Sex),
                  age=c(min(Orthodont$age),
                            max(Orthodont$age)))

library(ggplot2)
p <- ggplot(Orthodont, aes(x=age, y=distance, colour=Sex)) +
  geom_point(size=3) +
  geom_line(aes(y=predict(fm2), group=Subject, size="Subjects")) +
  geom_line(data=newdat, aes(y=predict(fm2, level=0, newdata=newdat), size="Population")) +
  scale_size_manual(name="Predictions", values=c("Subjects"=0.5, "Population"=3)) +
  theme_bw(base_size=22) 
print(p)

#//////////////# 
 
newdat <- expand.grid(retrofit_yes=unique(dat.mr[[1]]$retrofit_yes),
                  tmp_out=c(min(dat.mr[[1]]$tmp_out),
                            max(dat.mr[[1]]$tmp_out)))
 
p <- ggplot(dat.mr[[1]], aes(x=tmp_out, y=com_upper, colour=retrofit_yes)) +
  geom_point(size=3) +
  geom_line(aes(y=predict(m1c.lmer[[1]]), group=locID, size="Suites")) +
  geom_line(data=newdat , aes(y=predict(m1c.lmer[[1]], level=0, newdata=newdat), size="MURB 1 Population")) +
  scale_size_manual(name="Predictions", values=c("Suites"=0.2, "Population"=3)) +
  theme_bw(base_size=22) 
print(p)
 
summary(m1c.lmer[[1]]) 

summary(m1c.lmer[[1]])
anova(m1c.lmer[[1]])
fixef(m1c.lmer[[1]])


 # For lmer.1 For mr1, Notice locID explains 0.0599/(0.0599+0.0953) = 38.5% of variation that's "leftover" after the variance explained by our fixed effects... perform this calculation for each 
summary(m1.lmer[[1]])
 

# VISUALIZE
(mm_plot <- ggplot(inputData_mr1, aes(x = tmp_out, y = com_upper, colour = retrofit_yes)) +
      facet_wrap(~locID, nrow=2) +   # a panel for each suite
      geom_point(alpha = 0.2) +
      theme_classic() +
      geom_line(data = cbind(inputData_mr1, pred = predict(mixed.lmer1)), aes(y = pred), size = 1) +  # adding predicted line from mixed model 
      theme(legend.position = "bottom") +   
      scale_color_manual(values = c(colour2, colour1))+
      ylim(0,1)
)
 



```



```{r}

##### SURVEY DATA:

    # 7)  Organize survey data from pre and post: gather into one dataframe
    # 8)  Create a model for survey data

#Import raw data, from pre an post surveys:
file.surv.pre <- 
      read_xlsx(choose.files(),sheet = "Master", range = cell_cols("A:GK"))  #Pre-retrofit survey data:8203 Occupant Air Quality Study Analysis v2 
file.surv.post <- read_xlsx(choose.files(),sheet = "A1") # Post-retrofit survey data: 9816_Onsite_Final Data2_Cleaned


# Clean up raw data, e.g., Focus on relevant columns:
dat.surv.pre <-
    file.surv.pre[c(1,5:7,53:64)] %>% #Keep ResponseID, building, suite ID, and answers to thermal comfort questions
    rename(First_survey = First_Survey) %>%
    mutate (First_survey = TRUE)   #Make logical variable for pre/post distinction
dat.surv.post <- 
    file.surv.post[c(9:10,13,15,61:72)] %>% 
    rename (ResponseID = occupant_import_dat_ResponseID) %>%
    mutate (Second_survey = TRUE) %>% 
    rename (Building = AptNm) %>%  # rename to match pre-retrofit data 
    rename (Suite = occupant_import_dat_Resp_Info_Suite) %>%  # rename to match pre-retrofit data 
    mutate (Suite = str_pad(Suite, 4, pad = "0")) %>%  # format with leading zero if missing
    mutate (Suite_coded = chartr("0123456789", "abcdefghij",  # code the last two digits as alphas
                                 substr(Suite,3,4))) %>%
    mutate (CODE = paste0("mr",Building,"_", substr(Suite,1,2),Suite_coded)) # create CODE variable matching pre-retrofit

    
# Merge Survey data into one dataframe:
dat.surv.all <-
    merge(dat.surv.pre , dat.surv.post, 
          by = c("ResponseID","CODE","Building"), 
          all = TRUE) %>%
    subset(select= -c(Suite_coded,Suite,Building)) %>% # get ride of unused columns
    select(ResponseID,CODE,numbed,First_survey,Second_survey, everything()) #re-order columns for ease of reading


# Interpret the survey answers into categories of "Too Cold", "Too Warm" or "Just Rignt", or "Don't Know"
# Reminder of labels in survey codebook: Too cold==1, Just right==2, Too warm ==3, Don't Know ==4.
dat.surv.all <-
    dat.surv.all %>%
    mutate ( # Create a new column for respondants that are too warm, in the summer
        Pre_S_warm = case_when( 
            Q8a_Summer==3  ~ 1, # Bachelor Apartments
            Q8b_Summer_1==3 | Q8b_Summer_2== 3 ~ 1, # One bedroom Apartments
            Q8c_Summer_2==3 | Q8c_Summer_3==3 ~ 1, # Family Apartments
            is.na(First_survey) ~ NA_real_, # Make sure only counting pre-retrofit respondants
            TRUE ~ 0)) %>% # Add Zeros to rest
    mutate ( # Create a new column for respondants that are too cold, in the summer
        Pre_S_cold = case_when(
            Q8a_Summer==1  ~ 1, # Bachelor Apartments
            Q8b_Summer_1==1 | Q8b_Summer_2== 1 ~ 1, # One bedroom Apartments
            Q8c_Summer_2==1 | Q8c_Summer_3==1 ~ 1, # Family Apartments
            is.na(First_survey)  ~ NA_real_, # Make sure only counting pre-retrofit respondants
            TRUE ~ 0)) %>% 
    mutate ( # Create a new column for respondants that are just right, in the summer
        Pre_S_just.right = case_when(
            Q8a_Summer==2  ~ 1, # Bachelor Apartments
            Q8b_Summer_1==2 & Q8b_Summer_2== 2 ~ 1, # One bedroom Apartments
            Q8c_Summer_2==2 & Q8c_Summer_3==2 ~ 1, # Family Apartments
            is.na(First_survey)  ~ NA_real_, # Make sure only counting pre-retrofit respondants
            TRUE ~ 0))  %>%
    mutate ( # Create a new column for respondants that "don't know", in the summer
        Pre_S_DK = case_when(
            Q8a_Summer==4  ~ 1, # Bachelor Apartments
            Q8b_Summer_1==4 & Q8b_Summer_2== 4 ~ 1, # One bedroom Apartments
            Q8c_Summer_2==4 & Q8c_Summer_3==4 ~ 1, # Family Apartments
            is.na(First_survey) ~ NA_real_, # Make sure only counting pre-retrofit respondants
            TRUE ~ 0)) %>%
    mutate ( # Create a new column for respondants that are too warm, in the summer
        Post_S_warm = case_when( 
            S8A==3  ~ 1, # Bachelor Apartments
            S8Br1==3 | S8Br2== 3 ~ 1, # One bedroom Apartments
            S8Cr2==3 | S8Cr3==3 ~ 1, # Family Apartments
            is.na(Second_survey) ~ NA_real_, # Make sure only counting pre-retrofit respondants
            TRUE ~ 0))   # Add Zeros to rest





dat.surv.all %>%
  mutate(check_temp = Pre_S_cold+Pre_S_warm+Pre_S_just.right+Pre_S_DK) %>%
  select(check_temp)
   
 
 
 
Check_count_responses<-
  dat.surv.all %>%
  group_by(First_Survey, Second_survey) %>%
  summarize(Respondants = n_distinct(ResponseID))

##### END OF SURVEY ANALYSIS CHUNK
```




```{r}
##### ARCHIVED: EARLY IEQ MODEL EXPLORATION ============================

#Visualize the average temperature indoors as a function of outdoors: 
ggplot(dat.Sensor.Weather, aes(x=tmp_out, y=tmp_in, color=retrofit_yes)) +
  geom_point(alpha=0.05) + 
  geom_rug() +
  facet_grid(~ murb) +
  geom_smooth(method=lm)  # Add regression lines


#Visualize the average temperature indoors as a function of outdoors: 
ggplot(dat.Sensor.Weather, aes(x=tmp_out, y=tmp_in, color=retrofit_yes)) +
  geom_point(alpha=0.05) + 
  geom_rug() +
  facet_grid(~ murb) +
  geom_smooth(method=lm)  # Add regression lines
  

#Visualize the com_upper as a function of outdoor temp: 
dat.Sensor.Weather %>%
  filter(murb %in% c("mr1","mr2","mr6","mr7")) %>%
  ggplot(aes(x=tmp_out, y=com_upper, color=retrofit_yes)) +
  geom_point(alpha=0.05) + 
  geom_rug() +
  facet_grid(~ murb) +
  geom_smooth(method=lm)  # Add regression lines  
  
#family=quasibinomial
# e.g., model3 <- glm(Proportion ~ Vegetation, family = quasibinomial, data = aedes_dat)
# summary(model3)

dat.Sensor.Weather %>%
  filter(murb %in% c("mr1","mr2","mr6","mr7")) %>%
  ggplot(aes(x=tmp_out, y=com_upper, color=retrofit_yes)) +
  geom_point(alpha=0.05) + 
  geom_rug() +
  facet_grid(~ murb) +
  geom_smooth(method="glm",
              method.args = list(family = "quasibinomial"))
 
```


```{r}

##### BETA REGRESSION MODEL MIXED EFFECTS ============================
 

## Simulate some beta data...
set.seed(3);n<-400
dat <- gamSim(1,n=n)
mu <- binomial()$linkinv(dat$f/4-2)
phi <- .5
a <- mu*phi;b <- phi - a;
dat$y <- rbeta(n,a,b) 

#beta_model_mr1 <- gam(y~s(x0)+s(x1)+s(x2)+s(x3),family=betar(link="logit"),data=dat)
inputData_mr1 <-
    DF_SensorNWeather %>%
        filter(murb == "mr1")  
model_beta_mr1 <- gam(inputData_mr1$com_upper ~ s(inputData_mr1$retrofit_yes) + s(inputData_mr1$tmp_out) + s(inputData_mr1$locID), family=betar(link="logit"))
summary(model_beta_mr1)

bm
plot(bm,pages=1)




#e.g., glmmTMB(y ~ 1 + (1|pond), df, family=list(family="beta",link="logit"))
# glmmTMB is Generalized Linear Mixed Models using Template Model Builder

inputData <- 
    DF_SensorNWeather%>%
        mutate( # trasnform y=upper_com s.t. (y * (n−1) + 0.5) / n , where n is the sample size
           trans_com = (com_upper*(length(com_upper)-1)+0.5)/length(com_upper)) %>%
        subset(murb=="mr1")

glmmTMB(trans_com ~ tmp_out + retrofit_yes + (1|locID), inputData,
            family= beta_family(link = "logit"))
        
        
        glmmTMB(formula, data = NULL, family = gaussian(), ziformula = ~0,
dispformula = ~1, weights = NULL, offset = NULL,
contrasts = NULL, na.action = na.fail, se = TRUE,
verbose = FALSE, doFit = TRUE, control = glmmTMBControl(),
REML = FALSE)

 

```


```{r}
##### TWO-LIMIT OBIT MODEL ============================

inputData <- 
    dat.Sensor.Weather%>% 
        subset(murb=="mr1")
inputData_pre <- 
    dat.Sensor.Weather%>% 
        subset(murb=="mr1") %>%
        subset(retrofit_yes==FALSE)
inputData_post <- 
    dat.Sensor.Weather%>% 
        subset(murb=="mr1") %>%
        subset(retrofit_yes==TRUE)

summary(tobitMod <- vglm(com_upper ~ tmp_out + retrofit_yes, tobit(Lower=0, Upper = 1), data = dat.Sensor.Weather))

tobit_fit_pre <-vglm(com_upper ~ tmp_out, tobit(Lower=0, Upper = 1), data = inputData_pre) 
tobit_fit_post <-vglm(com_upper ~ tmp_out, tobit(Lower=0, Upper = 1), data = inputData_post)

ggplot() + 
    geom_point(data = inputData, size = 0.1, alpha = 0.5, aes(x = tmp_out, 
                                                              y = com_upper, 
                                                              color=retrofit_yes)) + 
    geom_line(data = inputData_pre, aes(x = tmp_out, 
                                        y = predict(tobit_fit_pre, type="response"), 
                                        colour = "Pre-retrofit")) + 
    geom_line(data = inputData_post, aes(x = tmp_out, 
                                        y = predict(tobit_fit_post, type="response"), 
                                        colour = "Post-retrofit")) +
    coord_cartesian(ylim=c(0, 1))

```


```{r}
##### OLD BETA REGRESSION MODEL FIXED ============================


# Try a Beta Regression Model *must transform to make com_upper ~ 0,1) open interval
inputData <- 
    dat.Sensor.Weather%>%
        mutate( # trasnform y=upper_com s.t. (y * (n−1) + 0.5) / n , where n is the sample size
           trans_com = (com_upper*(length(com_upper)-1)+0.5)/length(com_upper)) %>%
        subset(murb=="mr1")
inputData_pre <- 
    dat.Sensor.Weather%>%
        mutate( # trasnform y=upper_com s.t. (y * (n−1) + 0.5) / n , where n is the sample size
           trans_com = (com_upper*(length(com_upper)-1)+0.5)/length(com_upper)) %>%
        subset(murb=="mr1") %>%
        subset(retrofit_yes==FALSE)
inputData_post <- 
    dat.Sensor.Weather%>%
        mutate( # trasnform y=upper_com s.t. (y * (n−1) + 0.5) / n , where n is the sample size
           trans_com = (com_upper*(length(com_upper)-1)+0.5)/length(com_upper)) %>%
        subset(murb=="mr1") %>%
        subset(retrofit_yes==TRUE)

summary(inputData_post)
summary(inputData_pre)

betaMod_pre <- betareg(trans_com ~ tmp_out, data = inputData_pre)
betaMod_post <- betareg(trans_com ~ tmp_out, data = inputData_post) 

summary(betaMod_pre)
summary(betaMod_post)


ggplot(inputData, aes(x = tmp_out, y = trans_com)) +
    geom_point(size = 0.1, alpha = 0.5, aes(color=factor(retrofit_yes))) +
    geom_line(aes(y = predict(betaMod_pre, inputData),
                  colour = "pre", linetype = "pre")) +
    geom_line(aes(y = predict(betaMod_post, inputData),
                  colour = "post", linetype = "post")) + 
    scale_colour_manual("", values = c("dark red", "dark blue","red","blue"))  +
    scale_linetype_manual("", values = c("solid", "dashed")) # +
    #theme_bw()

```


```{r}
##### VOIB MODEL ============================
inputData <- 
    dat.Sensor.Weather%>%
        subset(murb=="mr1")

model <- zoib(com_upper ~ tmp_out + retrofit_yes|tmp_out|retrofit_yes|1,
              joint = FALSE, 
              zero.inflation = TRUE, 
              one.inflation = TRUE, 
              random = 0, 
              EUID = 1:nrow(inputData),  
              link.mu = "logit", 
              link.x0 = "logit", 
              link.x1 = "logit", 
              data = inputData)
# model summary 
sample1 <- model$coeff

# posterior inferences from zoib
summ1 <- summary(sample1)

summ2 <- cbind(summ1$stat[, 1], summ1$quant[, c(1, 5)])

paraplot(summ2, legpos = "topright", annotate=TRUE, legtext = c("zoib"))

# OR 
install.packages("gamlss")
install.packages("gamlss.dist")

require(gamlss)
require(gamlss.dist)

model.gamlss <- gamlss(racine2 ~ genotype + traitement, 
                sigma.formula=~genotype, 
                nu.formula=~traitement, 
                tau.formula=~1,
                family=BEINF)

summary(model.gamlss)

GAIC(model.gamlss)





```

 
```{r}
# EXTRANNEOUS TIDBITS ============================

sapply(Sensordata,class)

######### ARCHIVED original weekly sensor data

#SensorRawWeekly <- 
#    read.dta13(choose.files()) %>% # Load LTcomplete_weekly_collapsed.dta, which should be in a private folder i.e., NOT PUBLIC on Github
#    mutate( 
#        murb = substr(locID,1,3)) %>% # Create a column for building 
#    rename(week_midpt = date, tmp_in_wk = tmp, weekID = week) %>% # distinguish that date is midpoint of a weekly average 
#    select(murb,locID,weekID,week_midpt,tmp_in_wk,com_lower,com_upper) 

 
SensorSummary <-
    SensorCleaned %>%
    group_by(weekID, week_midpt) %>%
    summarise(n_obs_wks = n()) # ***FLAG: weeks are not consistent across murbs. Solution: need daily-level data and re-group into weeks.       

Check2<- # look at dates of sensorData...because we want to align the weather data with these dates
    SensorRaw %>%
    select(date) %>%
    distinct() # the sensor data skips a day at new years; plus not all murbs have same week midpoint...

# n.b. These next steps can be simplified if sensor data is daily; the next steps are a 'workaround' because
# the current sensor data is not continuously cut into weeks; rather it re-begins at the beginning of each calendar year,
# which means the current sensor data skips a day at the end of each year; furthermore, the sensor data weeks are not always the same for different murbs... preference would be to get the daily-level data here. For now: assign a week to weather data and match with 'closest' week among sensor data. 

Weather_Wkly <-
  # for every week_midpt in SensorClean, create a corresponding weekly average 
  for (i in SensorCleaned$week_midpt)
  
  #group_by(wks) %>%
  summarise(tmp_out_wkly = mean(tmp_out_daily)) %>%
  mutate(
        week_midpt = as.Date(wks) + 3) # to match with sensordata   


######### weather data to workaround misaligned sensor dating: (this works but is clunky) #########
date2015<-
    WeatherCleaned %>%
    subset(year(date) == 2015) %>%
    group_by(
        wks = cut(date, breaks = '7 days')) 
date2016<-
    WeatherCleaned %>%
    subset(year(date) == 2016) %>%
    group_by(
        wks = cut(date, breaks = '7 days')) 
date2017<-
    WeatherCleaned %>%
    subset(year(date) == 2017) %>%
    group_by(
        wks = cut(date, breaks = '7 days'))
date2018<-
    WeatherCleaned %>%
    subset(year(date) == 2018) %>%
    group_by(
        wks = cut(date, breaks = '7 days'))

dates_merged <-
  Reduce(function(x, y) merge(x, y, all=TRUE), list(date2015, date2016, date2017, date2018)) 

WeatherCleaned <-
  na.omit(dates_merged) %>%
  group_by(wks) %>%
  summarise(tmp_out_wkly = mean(tmp_out_daily)) %>%
  mutate(
        week_midpt = as.Date(wks) + 3) # to match with sensordata     

######### end of weather data workaround #########

IEQ <- 
    merge(WeatherCleaned,SensorCleaned,by="week_midpt", all.x = TRUE, all.y = TRUE) %>% # outdoor and indoor data into one dataframe
    select(locID,murb,week_midpt,tmp_out_wkly,tmp_in_wk,com_lower,com_upper) %>% #re-order and get rid of extra date info
    mutate(
        retrofit_yes = case_when (week_midpt %within% interval(ymd("2015-04-01"),ymd("2016-04-01"))~ FALSE,
                                  week_midpt %within% interval(ymd("2017-02-15"),ymd("2018-02-15"))~ TRUE,
                                  TRUE ~ NA)) %>% #default if not within the specified intervals  
    drop_na(retrofit_yes) %>%
    group_by(week_midpt) %>%
    summarise(n_obs_wks = n())
  

Pre_Post_count<-
  SensorCleaned %>%
      mutate(
        retrofit_yes = case_when (week_midpt %within% interval(ymd("2015-04-01"),ymd("2016-04-01"))~ FALSE,
                                  week_midpt %within% interval(ymd("2017-02-15"),ymd("2018-02-15"))~ TRUE,
                                  TRUE ~ NA)) %>% #default if not within the specified intervals  
    drop_na(retrofit_yes) %>%
    group_by(week_midpt) %>%
    summarise(n_obs_wks = n())


# Check3  
x<-SensorCleaned$week_midpt[1522]
y<-WeatherCleaned$week_midpt[52] 
x
y
all.equal(x,y) #Note, some dates e.g., Dec 27th 2015, January 4...  
# merging will not link sensor to weather data unless fix the misfit dates manually :(   
WeatherCleaned$week_midpt[52] <- SensorCleaned$week_midpt[1522]


  merge(SensorCleaned,WeatherCleaned, by="week_midpt", all.x = TRUE, all.y = TRUE) %>%
  subset(week_midpt %within% interval(as.Date("2015-12-25"), as.Date("2016-01-04")))
 
 


yr_list <- c(2015,2016)
test2 <- subset(WeatherCleaned,FALSE)
test2 <- for (yr in yr_list) {  
    WeatherCleaned %>% subset(year(date)==yr) %>% bind_rows(.,test2)
}

 #example  d %>% mutate( ints = cut(v1 ,breaks = 11)) %>% group_by(ints) 
    
# Can't use cut after the "case_when" call because the very first cut does the entire date range...
# Solution must use cut on only a portion of dates...need to replicate columns for each year, cut them each differently, then reassemble into one column and group_by on that 
WeatherCleaned$week_start <-
    #cut(WeatherCleaned$date,breaks = '7 days')  
    case_when(
            WeatherCleaned$date %within% interval(ymd("2015-01-01"),ymd("2015-12-31")) ~ 2015, 
            WeatherCleaned$date %within% interval(ymd("2016-01-01"),ymd("2016-12-31")) ~ 2016,
            TRUE ~ 2017)

WeatherCleaned$week_start <-
    #cut(WeatherCleaned$date,breaks = '7 days')  
    case_when(
            WeatherCleaned$date %within% interval(ymd("2015-01-01"),ymd("2017-12-31")) ~ cut(
                      WeatherCleaned$date + 0,
                      breaks = '7 days'), 
            WeatherCleaned$date %within% interval(ymd("2016-01-01"),ymd("2016-12-31")) ~ cut(
                      WeatherCleaned$date + 6,
                      breaks = '7 days'), 
            TRUE ~ cut(
                      WeatherCleaned$date + 5,
                      breaks = '7 days'))  

 mutate(
        week_start = case_when( # create a weekly average, to align with midpoint weekly weather station data
                          date %within% interval(ymd("2015-01-01"),ymd("2016-12-31")) ~ group_by(
                                      week_start = cut(
                                            date,
                                            date+0,
                                            breaks = '7 days')),
                          date %within% interval(ymd("2016-01-01"),ymd("2017-12-31")) ~ groub_by(
                                      week_start = cut(
                                            date,
                                            date-1,
                                            breaks = '7 days')),
                          TRUE ~ "placeholder")) %>%
    summarise(tmp_out_wk = mean(tmp_out_daily)) %>%
    mutate(
        week_midpt = as.Date(week_start) + 3) # to match with sensordata                         
      
  
 
  
    group_by(week_start = case_when(   # create a weekly average, to align with midpoint weekly weather station data
                      date %within% interval(ymd("2015-04-01"),ymd("2016-04-01"))~ cut(
                                                                                            week_start,
                                                                                            date +0,
                                                                                            breaks = '7 days'),
                      date %within% interval(ymd("2017-02-15"),ymd("2018-02-15"))~ cut(
                                                                                            week_start,
                                                                                            date +5,
                                                                                            breaks = '7 days')))        

      

```
 